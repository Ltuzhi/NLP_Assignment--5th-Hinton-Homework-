{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出于1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要首先定义自己的语言。 可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言环境介绍：\n",
    "\n",
    "  当今社会，人们的生活节奏越来越快，压力越来越大，所以常常感觉身心疲惫，有时还会遇到总也解决不了的困境。这个时候，往往一句鼓励的、表扬的或者肯定的话语，就会让人觉得倍感欣慰，感觉一切努力仍是值得的。所以现在也出现了一些“夸夸群”，某宝上甚至可以购买这类夸夸群，该群的目的就是里面的人会不停地夸奖顾客，用夸张的方式把顾客夸的天花乱坠，美曰“彩虹屁”。这种方式虽然夸张，但是从某种层面的反应出了人们渴求得到认可。所以该语法名为**“夸夸语”**，目的就是当你感觉到疲惫，难受，渴望被肯定的时候，让计算机生成一系列夸奖自己的语言，娱乐放松一下。\n",
    "\n",
    "有希望得到肯定的诉求，那么相反的，也会有想要听到严肃警醒的话语。生活中有着各种各样的诱惑，稍微一不注意，就会被欲望侵蚀，会阻碍我们向目标迈进，比如想要减肥的你，今天下班正好赶上公司聚餐，面对一大桌美味佳肴，你一定会动摇昨天想要减肥的心。又或者太过安逸的环境，就像是温水煮青蛙，你感觉不到危机的存在，却是一点点在消磨你的生命。这个时候，人们就会希望有人可以一巴掌把自己拍醒，通过严厉的警示话语，刺激和提醒自己，要经得起诱惑，要奋起直追，不可以像现在这样懒惰，不积极，没志向。所以诞生了该语法**“警示语”**，这是与“夸夸语”完全相反的存在，前者让人感觉如沐春风，后者感觉像是被人一棒子打醒。每个人每个阶段和状态不同，偶尔需要被夸奖，偶尔也需要被警示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义语法：\n",
    "\n",
    "第一个语法：“夸夸语”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "praise_rules = '''\n",
    "praise sb. -> name degrees praise tail over\n",
    "name -> 你 | 这 | 亲 | 小仙女 | 小可爱 | 小姐姐 | 小哥哥 | 小姑娘 | 小伙子 | 亲爱的\n",
    "degrees -> degree degrees | degree\n",
    "degree -> 很 | 非常 | 真 | 太 | 真的 | 真是 | 简直\n",
    "praise -> 棒 | 美 | 帅 | 厉害 | 不错 | 可爱 | 聪明 | 机智 | 漂亮 | 优秀 | 能干 | 辛苦\n",
    "tail ->   | 呀 | 啦 | 了 | 哦 | 啊\n",
    "over -> 。 | ！ | ~\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法：“警示语”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caution_rules = '''\n",
    "caution sb. -> names degree caution over\n",
    "names -> name names | name\n",
    "name -> 你 | 喂 | 猪头 | 同学 | 姑娘 | 先生 | 小姐 | 大懒猪 | 大胖子 | 瞌睡虫 | 特困生 | 大笨蛋\n",
    "degree -> 该 | 要 | 赶快 | 应该 | 真该 | 快点 | 需要\n",
    "caution -> 醒醒 | 别吃 | 减肥 | 喝水 | 上进 | 学习 | 工作 | 早睡 | 锻炼 | 敲代码 | 提高效率\n",
    "over -> 了 | ！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation_by_gram(grammar_str: str, target, stmt_split='->', expr_split='|'):\n",
    "\n",
    "    rules = dict() # key is the @statement, value is @expression\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "        # 跳过空白行\n",
    "        # print(line)\n",
    "        stmt, expr = line.split(stmt_split) # 分隔 statement 和 expression\n",
    "    \n",
    "        rules[stmt.strip()] = expr.split(expr_split) # 分隔 express 中的词\n",
    "    \n",
    "    generated = generate(rules, target=target)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammar_rule, target):\n",
    "    if target in grammar_rule: # names or degrees\n",
    "        candidates = grammar_rule[target]  # ['name names', 'name']\n",
    "        candidate = random.choice(candidates) #'name names', 'name'\n",
    "        return ''.join(generate(grammar_rule, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小姐姐真是可爱啊。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generation_by_gram(praise_rules,target='praise sb.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 生成直接打印的 n 个句子（不够有通用性）\\ndef generate_n(num: int, grammar_name, grammer_target:str): \\n    for n in range(num):\\n        print(n+1,'. ',get_generation_by_gram(grammar_name,target=grammer_target))\\n    pass\\n# 使用：\\ngenerate_n(10,praise_rules,'praise sb.') # 打印 10 个夸夸句\\ngenerate_n(10,caution_rules,'caution sb.') # 打印 10 个警示句\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 生成直接打印的 n 个句子（不够有通用性）\n",
    "def generate_n(num: int, grammar_name, grammer_target:str): \n",
    "    for n in range(num):\n",
    "        print(n+1,'. ',get_generation_by_gram(grammar_name,target=grammer_target))\n",
    "    pass\n",
    "# 使用：\n",
    "generate_n(10,praise_rules,'praise sb.') # 打印 10 个夸夸句\n",
    "generate_n(10,caution_rules,'caution sb.') # 打印 10 个警示句\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将生成的 n 个句子放入数组中\n",
    "def generate_m(num: int, grammar_name, grammer_target:str): \n",
    "    m = []\n",
    "    for n in range(num):\n",
    "        m.append(get_generation_by_gram(grammar_name,target=grammer_target))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小姐姐真是棒啊！\n",
      "你真是真的聪明了~\n",
      "小姑娘真是太可爱哦！\n",
      "这很简直真漂亮。\n",
      "小仙女简直很聪明哦！\n",
      "你简直能干！\n",
      "你太美哦~\n",
      "这真真的可爱哦！\n",
      "小哥哥很简直机智了~\n",
      "小姑娘简直真的漂亮啦！\n"
     ]
    }
   ],
   "source": [
    "# 打印 10 个夸夸句\n",
    "for i in generate_m(10,praise_rules,'praise sb.'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特困生快点锻炼了\n",
      "猪头先生需要喝水了\n",
      "喂需要敲代码！\n",
      "猪头该学习！\n",
      "瞌睡虫猪头你小姐应该锻炼！\n",
      "瞌睡虫赶快上进了\n"
     ]
    }
   ],
   "source": [
    "# 打印 6 个警示句\n",
    "for i in generate_m(6,caution_rules,'caution sb.'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照定义的`prob_2`函数（即：$ Pr(sentence) = Pr(w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i, w_{i+1})}{count(w_{i+1})}$），我们**更换一个文本数据源**，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 读取数据，进行数据清洗，获取纯文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'dataset/Douban comments.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FILE = open(corpus, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,link,name,comment,star\\n1,https://movie.douban.com/subject/26363254/,战狼2,吴京意淫到了脑残的地步，看了恶心想吐,1\\n2,https://movie.douban.com/subject/26363254/,战狼2,首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹,2\\n3,https://movie.douban.com/subject/26363254/,战狼2,吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留汉字,其中 [\\u4e00-\\u9fa5] 为汉字的utf-8 code范围\n",
    "def remove_others(s):    \n",
    "    return re.sub(r'[^\\u4e00-\\u9fa5]', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 纯文本语料库\n",
    "newFILE = remove_others(FILE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'战狼吴京意淫到了脑残的地步看了恶心想吐战狼首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹战狼吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家更新片子整体不如湄公河行动整体不够流畅编剧有毒台词尴尬刻意做作的主旋律煽情显得如此不合时宜而又多余战狼凭良心说好看到不像战狼的续集完虐湄公河行动战狼中二得很战狼犯我中华者虽远必诛吴京比这句话还要意淫一百倍战狼脑子是个好东西希望编剧们都能有战狼三星半实打实的分第一集在爱国主旋律内部做着各种置换与'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFILE[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39344"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 对文本进行切词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.161 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "TOKENS = cut(newFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22703"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['战狼',\n",
       " '吴京',\n",
       " '意淫',\n",
       " '到',\n",
       " '了',\n",
       " '脑残',\n",
       " '的',\n",
       " '地步',\n",
       " '看',\n",
       " '了',\n",
       " '恶心',\n",
       " '想吐战',\n",
       " '狼',\n",
       " '首映礼',\n",
       " '看',\n",
       " '的',\n",
       " '太',\n",
       " '恐怖',\n",
       " '了',\n",
       " '这个']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram 词频记录\n",
    "words_count = Counter(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 1283),\n",
       " ('了', 532),\n",
       " ('和', 322),\n",
       " ('是', 287),\n",
       " ('我', 285),\n",
       " ('战狼', 245),\n",
       " ('来', 245),\n",
       " ('幸福', 219),\n",
       " ('李雷', 207),\n",
       " ('敲门', 205),\n",
       " ('当', 198),\n",
       " ('韩梅梅', 197),\n",
       " ('都', 177),\n",
       " ('看', 158),\n",
       " ('在', 154),\n",
       " ('不', 154),\n",
       " ('就', 150),\n",
       " ('电影', 146),\n",
       " ('有', 135),\n",
       " ('也', 122)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 查看词频分布情况\\n\\nwords_with_fre = [f for w,f in words_count.most_common()]\\nimport matplotlib.pyplot as plt\\nwords_with_fre[:10]\\nimport numpy as np\\nplt.plot(np.log(words_with_fre)) # 画图，log 平滑操作\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 查看词频分布情况\n",
    "\n",
    "words_with_fre = [f for w,f in words_count.most_common()]\n",
    "import matplotlib.pyplot as plt\n",
    "words_with_fre[:10]\n",
    "import numpy as np\n",
    "plt.plot(np.log(words_with_fre)) # 画图，log 平滑操作\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. 建立 2-gram 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['战狼吴京', '吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2_gram_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram 词频记录\n",
    "_2_gram_word_counts = Counter(_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2_gram_word_counts.most_common()[-1][-1] # 查看最后一组词的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求词语在语料库中的频率\n",
    "def get_gram_count(word,wc):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1] # 若输入词语不在语料库中，则赋予最小的词频数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 2-gram 语言模型\n",
    "def two_gram_model(sentence):\n",
    "    \n",
    "    tokens = cut(sentence) # 对传入句子分词\n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        _two_gram_c = get_gram_count(word+next_word,_2_gram_word_counts) # 计算 2-gram(word+next_word) 词组在语料库中的频率\n",
    "        # print('1.', _two_gram_c)\n",
    "        _one_gram_c = get_gram_count(next_word,words_count) # 计算 1-gram(next_word) 在语料库中的频率\n",
    "        # print('2.', _one_gram_c)\n",
    "        pro = _two_gram_c / _one_gram_c\n",
    "        \n",
    "        probability *= pro # 计算概率\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.08386533222674e-07"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "two_gram_model('很好今天天气，很好心情也！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3408110140116258e-08"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "two_gram_model('今天天气很好，心情也很好！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 关于 sorted 函数用法\\nsorted([1, 3, 5, 2]) => [1, 2, 3, 5]\\n# 这个函数接受一个参数key，这个参数接受一个函数作为输入，例如\\n# 能够让list按照第0个元素进行排序.\\nsorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0]) => [(1, 4), (2, 5), (4, 4), (5, 0)]\\n# 能够让list按照第1个元素进行排序.\\nsorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1]) =>  [(5, 0), (1, 4), (4, 4), (2, 5)]\\n# 能够让list按照第1个元素进行排序, 但是是递减的顺序。\\nsorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True) => [(2, 5), (1, 4), (4, 4), (5, 0)]\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 关于 sorted 函数用法\n",
    "sorted([1, 3, 5, 2]) => [1, 2, 3, 5]\n",
    "# 这个函数接受一个参数key，这个参数接受一个函数作为输入，例如\n",
    "# 能够让list按照第0个元素进行排序.\n",
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0]) => [(1, 4), (2, 5), (4, 4), (5, 0)]\n",
    "# 能够让list按照第1个元素进行排序.\n",
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1]) =>  [(5, 0), (1, 4), (4, 4), (2, 5)]\n",
    "# 能够让list按照第1个元素进行排序, 但是是递减的顺序。\n",
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True) => [(2, 5), (1, 4), (4, 4), (5, 0)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(amount, grammar_name, grammer_target:str, model): \n",
    "    '''\n",
    "   amount: the number of sentences\n",
    "   grammer_name: There are two grammers, you can choose: praise_rules and caution_rules\n",
    "   grammer_target: The target of grammer : praise sb. and caution sb.\n",
    "   model: You can choose one model: two_gram_model\n",
    "    '''\n",
    "    \n",
    "    #生成 n 个句子\n",
    "    sentences = generate_m(amount,grammar_name,grammer_target)\n",
    "    \n",
    "    #计算 n 个句子的概率值\n",
    "    pro = []\n",
    "    for i in sentences:\n",
    "        pro.append(model(i))\n",
    "\n",
    "    # 合并为二维数组\n",
    "    result = list(zip(sentences,pro))\n",
    "    sorted_result = sorted(result, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    #输出结果\n",
    "    print('语法生成的', amount, '个句子为：')\n",
    "    for i in sentences:\n",
    "        print(i)\n",
    "        \n",
    "    print('-'*20)\n",
    "    print('其中最合理的句子：', sorted_result[0][0], ' 其概率：', sorted_result[0][1])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语法生成的 10 个句子为：\n",
      "小姑娘非常太棒啊！\n",
      "这真非常能干哦~\n",
      "亲很非常真的美了！\n",
      "亲真是辛苦呀。\n",
      "亲非常很真的简直可爱。\n",
      "小仙女真是真的能干哦~\n",
      "小可爱真简直厉害哦~\n",
      "亲爱的太辛苦哦。\n",
      "小可爱真辛苦哦。\n",
      "小哥哥非常真的太太厉害。\n",
      "--------------------\n",
      "其中最合理的句子： 亲真是辛苦呀。  其概率： 0.041666666666666664\n"
     ]
    }
   ],
   "source": [
    "# “夸夸句”\n",
    "generate_best(10,praise_rules,'praise sb.',two_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语法生成的 5 个句子为：\n",
      "小姐大懒猪需要锻炼了\n",
      "大懒猪快点敲代码了\n",
      "先生该学习！\n",
      "大胖子大笨蛋小姐快点提高效率了\n",
      "小姐特困生该喝水了\n",
      "--------------------\n",
      "其中最合理的句子： 先生该学习！  其概率： 0.02857142857142857\n"
     ]
    }
   ],
   "source": [
    "# “警示句”\n",
    "generate_best(5,caution_rules,'caution sb.',two_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "**存在的问题：** \n",
    "1. 语料库不够丰富，导致有些词语在语料库中检索不到，影响结果，可能对的句子比错的句子概率值更低低或者句子概率值等于 1.\n",
    "2. 词语与词语之间的关联程度不够，上下文联系性不够高，没有充分利用语料库资源。\n",
    "\n",
    "**提升：** \n",
    "1. 增加模型复杂度和上下文的关联程度，改为 3-gram 或者 5-gram 等，即增加目标词与之相关联的词语个数。\n",
    "2. 扩大语料库，使其能尽可能多的包含各种语言类型及内容。\n",
    "3. 完善和丰富语法模型，使其输出尽可能正确的句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
