{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Theoretical part （理论部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ans：每个神经元要经过两次计算，（linear + activation function（非线性）） 线性 + 激活函数（非线性）。\n",
    "线性可以理解为对分量加权重，激活函数通常是非线性的一个函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ans：\n",
    "1. 增加模型复杂度，使得神经网络能较好的拟合数据。\n",
    "2. 确保每一层都有意义，若每层只有线性权重，没有激活函数，那层与层之间就没有任何意义，多个线性操作可以整理到一个线性操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans：Logistic regression（逻辑回归）的 Loss(error) function（损失函数），逻辑回归相当于一层神经网络，因为是二分类问题，所以监督值 y 只有两个类型 0 和 1，模型算出来后也应该接近 0 和 1 这两种值，因此 Logistic Loss 函数定义如下：\n",
    "$$ L(y,\\hat{y}) = -(y \\text{log}(\\hat{y})+(1-y)\\text{log}(1-\\hat{y})) $$\n",
    "因为我们寻找最优解时，用到的是梯度下降法，该方法求得是最小值，所以我们加负号是为了符合函数要求。（不加负号时，这个函数实际上是可以得到 y 的值，我们要让这个函数尽可能最大，接近于 1 ，也就是加完负号后的函数尽可能小，接近于 0. 这样就可以用梯度方法，进行求解。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ans： 选 C\n",
    "\n",
    "激活函数应该选取是 sigmoid 函数，因为是二分类问题，所以监督值 y 只有两个类型 0 和 1，使用 sigmoid 函数可以将结果值映射到 [0,1] 之间，也可以理解为某一类的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ans：简单来说，如果所有参数的初始值都设为 0，那么在正向传播时，隐藏层中每个节点的值都相同，最后输出的值也是固定值。在反向传播更新权重的过程中，更新后的每个节点的参数也相同，那么不管迭代多少轮，得到的参数都是一样的，因此神经网络就失去了学习的意义。\n",
    "（具体推算过程可参考 https://www.cnblogs.com/lky-learning/p/10830223.html ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ans：softmax 函数公式如下：\n",
    "$$ \\sigma{(z)}_j = \\frac{e^{z_j}}{\\sum^K_{k=1} e^{z_k}}$$\n",
    "softmax 用于多分类过程，将结果映射到 (0,1) 区间，这些值的和为 1，可以理解成概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义softmax 函数\n",
    "def softmax(z):\n",
    "    \n",
    "    # 应该还需要考虑exp 溢出的情况，可以在分子上减去一个最大值，此处暂时不考虑。\n",
    "    \n",
    "    z_exp = np.exp(z) # 每个节点取 e 指数，即分子\n",
    "    sum_z = np.sum(z_exp) # 每个取完 e 指数之后的和，即函数的分母\n",
    "    \n",
    "    return z_exp / sum_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.71828183,  7.3890561 , 20.08553692, 54.59815003])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27182818, 0.73890561, 2.00855369, 5.459815  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x)/np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 随机取一组数据，得到其 softmax 后的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "z = np.random.random((2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0225499 , 0.74236351, 0.21675808, 0.61249949, 0.42663081],\n",
       "       [0.71673325, 0.38796157, 0.61522458, 0.85719087, 0.03611334]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06195839, 0.12726564, 0.07523911, 0.11176658, 0.09280902],\n",
       "       [0.12404524, 0.08928867, 0.11207157, 0.1427513 , 0.06280448]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
