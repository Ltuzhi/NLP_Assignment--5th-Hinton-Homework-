{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - Theoretical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compared to FNN, what is the biggest advantage of CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans: CNN可以有效降低参数的数量，更适合处理图像这类高纬度的数据。在处理图像上，对于FNN，输入的维度较高，需要学习的参数多，容易过拟合。而CNN可以降低过拟合的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Suppose your input is a 100 by 100 gray image, and you use a convolutional layer with 50 filters that are each 5x5. How many parameters does this hidden layer have (including the bias parameters)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans: $$ (5*5*1+1)*50 = 1300 $$\n",
    "其中，$5*5$是filter维度，$5*5*1$中的1是因为灰度图像信道只有1个，$+1$是偏置参数，最后乘的是filter的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What are \"local invariant\" and \"parameter sharing\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans: \n",
    "1. \"local invariant\": 平移不变性，是指当filter可以识别图像中一个特征的时候，当这个特征在图像中变换了位置，filter仍然可以识别出来。\n",
    "2. \"parameter sharing\" ： 参数共享，一张图片共同使用一个filter中的参数。可以以量级的降级参数的个数，降低模型复杂度，有效防止过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why we use batch normalization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans: batch normalization 即批标准化，简单说就是将数据集转化成均值为0，方差为1的矩阵。这样做的目的是\n",
    "1. 可以使每一层的分布都是相似的，都是均值为0方差为1的比较标准的正态分布，保持每层尺度一致，这样可以使用较高的学习率，使得收敛速度更快。\n",
    "2. 可以使非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题，保证网络可以不断训练。\n",
    "\n",
    "（还有移除或使用较低的dropout；降低L2权重衰减系数；取消Local Response Normalization层；减少图像扭曲的使用等作用。具体可以参考以下两篇博客，https://blog.csdn.net/qq_40168949/article/details/85047953 and https://blog.csdn.net/guo1988kui/article/details/83794343 ）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What problem does dropout try to solve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans: 神经网络在学习时，容易出现过拟合，为了降低过拟合的情况，可以使用\"dropout\"，即每次迭代都随机去掉一些连接，这样可以降低模型参数和复杂度，减少过拟合的现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  Is the following statement correct and why ? \"Because pooling layers do not have parameters, they do not affect  the backpropagation(derivatives) calculation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ans:这种说法是不正确的， \"pooling layers\" 虽然没有引入参数，但是在做反向传播的时候，遇到pooling层，为了逐层进行梯度的传播，我们仍然需要对pooling进行反向传播（求导）的计算。具体计算：\"max pooling\": 下一层的梯度会原封不动地传到上一层最大值所在位置的神经元，其他位置的梯度为0；\n",
    "\"average pooling\": 下一层的梯度会平均地分配到上一层的对应相连区块的所有神经元。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
